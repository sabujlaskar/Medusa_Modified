srun -p gpu-a100 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --gres=gpu:a100:1 --mem=80G --time=04:00:00 --pty bash -i
module load WebProxy GCCcore/12.2.0 Python/3.10.8
source venv/bin/activate
export TMPDIR="${SLURM_TMPDIR:-/tmp}"
export PYTHONPYCACHEPREFIX="$TMPDIR/__pycache__"
export XDG_CACHE_HOME="$TMPDIR/.cache"
export TORCH_HOME="$TMPDIR/torch"
export HF_HOME="$TMPDIR/huggingface"
export PIP_CACHE_DIR="$SCRATCH/.cache/pip"
export PIP_CACHE_DIR=$SCRATCH/.cache/pip

python -m pip install -U pip setuptools wheel
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126
pip install "transformers>=4.34,<5.0" accelerate sentencepiece "protobuf>=3.20,<5.0" fschat

CUDA_VISIBLE_DEVICES=0 python -m medusa.inference.cli --model FasterDecoding/medusa-1.0-vicuna-7b-v1.5

CUDA_VISIBLE_DEVICES=0 python -m medusa.inference.batch_cli --model FasterDecoding/medusa-1.0-vicuna-7b-v1.5 --input-file input.jsonl --output-file output.jsonl


module load WebProxy GCC/12.3.0 OpenMPI/4.1.5 Python/3.11.3 PyTorch/2.1.2-CUDA-12.1.1 Transformers/4.39.3 accelerate/0.33.0-CUDA-12.1.1 SentencePiece/0.2.0

print("Torch:", torch.__version__, "CUDA runtime:", torch.version.cuda, "CUDA available:", torch.cuda.is_available())

pip install "protobuf>=3.20,<5.0" fschat

# export TRANSFORMERS_CACHE=$SCRATCH/huggingface_cache

export HF_HOME=$SCRATCH/huggingface_cache
mkdir -p HF_HOME